# -*- coding: utf-8 -*-
import scrapy
from exploitdb.items import ExploitdbItem

class Edb1Spider(scrapy.Spider):
    name = "edb1"
    allowed_domains = ["exploit-db.com"]
    start_urls = ['https://www.exploit-db.com/google-hacking-database/?action=search&ghdb_search_page=1&ghdb_search_text=&ghdb_search_cat_id=0']

    def parse(self, response):
        # print('Got a response from %s.' % response.url)
        for href in response.xpath('//table/tbody/tr/td[2]/a/@href').extract():
            yield scrapy.Request(response.urljoin(href),
                                 callback=self.parse_mainsrc)

        next_page = response.xpath("//div[@class='pagination']/a[contains(.,'next')]/@href").extract_first()
        if next_page is not None:
            next_page = response.urljoin(next_page)
            yield scrapy.Request(next_page, callback=self.parse)

    def parse_mainsrc(self, response):
        yield {
        'Description': response.xpath("//table[@class='category-list']/tbody/tr[4]/td//text()").re('[^ \n].*[^ \n]')[1:],
        'Date': response.xpath("//table[@class='category-list']/tbody/tr[3]/td//text()").extract()[2],
        'Title': response.css('.l-titlebar h1::text').extract(),
        # item['Category': response.css('.gd-description a::text').re('[^ ]\w.*[^ ]')
        # item['Link2': response.css('.gd-description a::attr(href)').extract()
        #i['domain_id': response.xpath('//input[@id="sid"]/@value').extract()
        #i['name': response.xpath('//div[@id="name"]').extract()
        #i['description': response.xpath('//div[@id="description"]').extract()
        # item['Date': response.css('td.date::text').extract() #main page
        # item['Title': response.css('table.category-list td a::text').re('\w+.*[^ ]')
        }
